@InProceedings{le14,
  title = {Distributed Representations of Sentences and Documents},
  author = {Quoc Le and Tomas Mikolov},
  pages = {1188-1196},
  abstract = {Many machine learning algorithms require the  input to be represented as a fixed length feature  vector. When it comes to texts, one of the most  common representations is bag-of-words. De-  spite their popularity, bag-of-words models have  two major weaknesses: they lose the ordering  of the words and they also ignore semantics of  the words. For example, Òpowerful,Ó ÒstrongÓ  and ÒParisÓ are equally distant. In this paper,  we propose an unsupervised algorithm that learns  vector representations of sentences and text doc-  uments. This algorithm represents each docu-  ment by a dense vector which is trained to predict  words in the document. Its construction gives our  algorithm the potential to overcome the weak-  nesses of bag-of-words models. Empirical re-  sults show that our technique outperforms bag-  of-words models as well as other techniques for  text representations. Finally, we achieve new  state-of-the-art results on several text classifica-  tion and sentiment analysis tasks.},
  section = {cycle-2},
}
