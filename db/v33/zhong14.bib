@InProceedings{zhong14,
  title = {{Accelerated Stochastic Gradient Method for Composite Regularization}},
  author = {Zhong, Wenliang and Kwok, James},
  pages = {1086-1094},
  abstract = {Risk minimization often involves non-smooth optimization. This can be challenging when the regularizer is a composite of functions, as in overlapping group lasso. Very recently, the proximal average technique, which employs an implicit non-smooth function to approximate the original one, is introduced to overcome this problem. In this paper, we propose a novel accelerated gradient method  for stochastic optimization  based on the proximal average (PA-ASGD). We show that the resultant approximation error reduces at the rate O(1/N^{3/2}) and O((log N)/N^2) for general convex and strongly convex problems. This is better than the O(1/N) rate obtained by stochastic smoothing or the ADMM-based approaches.  Extensive experiments on a number of synthetic and real-world data sets demonstrate that PA-ASGD is faster than the state-of-the-art methods.},
}
